---
title: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
authors: ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"]
year: 2019
venue: "NAACL-HLT"
type: "paper"
cover: "../../assets/bert-cover.jpg"
links:
  pdf: "https://arxiv.org/abs/1810.04805"
  code: "https://github.com/google-research/bert"
  website: ""
  demo: ""
  slides: ""
  video: ""
badges:
  - { text: "Best Paper", type: "gold" }
description: "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers."
featured: true
---
We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.