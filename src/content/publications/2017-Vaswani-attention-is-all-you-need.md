---
title: "Attention is all you need"
authors: ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "\Lukasz Kaiser", "Illia Polosukhin"]
year: 2017
venue: "Advances in neural information processing systems"
type: "paper"
cover: "../../assets/transformer-cover.jpg"
links:
  pdf: "https://arxiv.org/pdf/1706.03762.pdf"
  code: "https://github.com/tensorflow/tensor2tensor"
  website: ""
  demo: ""
  slides: "https://www.slide-share.com/vaswani/attention"
  video: "https://www.youtube.com/watch?v=rBCqOTEfxvg"
badges:
  - { text: "Best Paper", type: "gold" }
description: "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely."
featured: true
---
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.